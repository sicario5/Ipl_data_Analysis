{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6089ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import io\n",
    "from collections import defaultdict\n",
    "from kafka import KafkaConsumer\n",
    "from s3fs import S3FileSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea58b4",
   "metadata": {},
   "source": [
    "Kafka Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731181f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BROKER = 'localhost:9092'\n",
    "TOPIC_NAME = 'csv_json_topic'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7f4be",
   "metadata": {},
   "source": [
    "S3 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a730642",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET = 'ipl_data'\n",
    "S3_PREFIX = f\"s3://{S3_BUCKET}/\"\n",
    "BATCH_SIZE = 100  # no. of row per file before flush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de62f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    TOPIC_NAME,\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    value_deserializer=lambda v: json.loads(v.decode('utf-8')),\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9da69c",
   "metadata": {},
   "source": [
    "S3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bdab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_buffers = defaultdict(list)\n",
    "headers_written = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d0c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŸ¢ Kafka consumer with batching started...\")\n",
    "\n",
    "try:\n",
    "    for message in consumer:\n",
    "        value = message.value\n",
    "        filename = value['filename']\n",
    "        row = value['data']\n",
    "\n",
    "        # Add row to buffer\n",
    "        file_buffers[filename].append(row)\n",
    "\n",
    "        # Flush if batch is full\n",
    "        if len(file_buffers[filename]) >= BATCH_SIZE:\n",
    "            s3_path = f\"{S3_PREFIX}{filename}\"\n",
    "            buffer = io.StringIO()\n",
    "            writer = csv.DictWriter(buffer, fieldnames=row.keys())\n",
    "\n",
    "            # Write header once per file\n",
    "            if filename not in headers_written:\n",
    "                writer.writeheader()\n",
    "                headers_written.add(filename)\n",
    "\n",
    "            writer.writerows(file_buffers[filename])  # write batch\n",
    "            with s3.open(s3_path, 'a') as s3_file:\n",
    "                s3_file.write(buffer.getvalue())\n",
    "\n",
    "            print(f\"âœ… Flushed {len(file_buffers[filename])} rows to {s3_path}\")\n",
    "            file_buffers[filename] = []  # clear buffer\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nðŸ›‘ Interrupted. Flushing remaining rows...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62d1c9",
   "metadata": {},
   "source": [
    "final flush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, rows in file_buffers.items():\n",
    "    if rows:\n",
    "        s3_path = f\"{S3_PREFIX}{filename}\"\n",
    "        buffer = io.StringIO()\n",
    "        writer = csv.DictWriter(buffer, fieldnames=rows[0].keys())\n",
    "\n",
    "        if filename not in headers_written:\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerows(rows)\n",
    "        with s3.open(s3_path, 'a') as s3_file:\n",
    "            s3_file.write(buffer.getvalue())\n",
    "\n",
    "        print(f\"ðŸŸ¡ Final flush: {len(rows)} rows to {s3_path}\")\n",
    "\n",
    "print(\"âœ… Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
